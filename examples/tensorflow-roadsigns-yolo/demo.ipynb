{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Road Signs YOLO Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">⚠️ **Warning:** This demo assumes that you have access to an on-prem deployment of Dioptra that provides a copy of the Road Signs dataset and a CUDA-compatible GPU.\n",
    "> This demo cannot be run on a typical personal computer.\n",
    "\n",
    "The demo provided in the Jupyter notebook `demo.ipynb` contains an example of how to set up and train a model based on the YOLO v1 architecture and use it to perform object detection on the Road Signs dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages from the Python standard library\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Please enter custom username here.\n",
    "USERNAME = \"jglasbrenner\"\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Address for connecting the docker container to exposed ports on the host device\n",
    "HOST_DOCKER_INTERNAL = \"host.docker.internal\"\n",
    "# HOST_DOCKER_INTERNAL = \"172.17.0.1\"\n",
    "\n",
    "# Testbed API ports\n",
    "RESTAPI_PORT = \"30080\"\n",
    "MLFLOW_TRACKING_PORT = \"35000\"\n",
    "\n",
    "# Default address for accessing the RESTful API service\n",
    "RESTAPI_ADDRESS = (\n",
    "    # f\"http://{HOST_DOCKER_INTERNAL}:{RESTAPI_PORT}\"\n",
    "    f\"http://restapi:{RESTAPI_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{RESTAPI_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the AI_RESTAPI_URI variable, used to connect to RESTful API service\n",
    "os.environ[\"AI_RESTAPI_URI\"] = RESTAPI_ADDRESS\n",
    "\n",
    "# Default address for accessing the MLFlow Tracking server\n",
    "MLFLOW_TRACKING_URI = (\n",
    "    # f\"http://{HOST_DOCKER_INTERNAL}:{MLFLOW_TRACKING_PORT}\"\n",
    "    f\"http://mlflow-tracking:{MLFLOW_TRACKING_PORT}\"\n",
    "    if os.getenv(\"IS_JUPYTER_SERVICE\")\n",
    "    else f\"http://localhost:{MLFLOW_TRACKING_PORT}\"\n",
    ")\n",
    "\n",
    "# Override the MLFLOW_TRACKING_URI variable, used to connect to MLFlow Tracking service\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
    "\n",
    "# Path to custom task plugins archives\n",
    "CUSTOM_PLUGINS_EVALUATION_TAR_GZ = Path(\"custom-plugins-evaluation.tar.gz\")\n",
    "CUSTOM_PLUGINS_ROADSIGNS_YOLO_ESTIMATORS_TAR_GZ = Path(\"custom-plugins-roadsigns_yolo_estimators.tar.gz\")\n",
    "\n",
    "# Base API address\n",
    "RESTAPI_API_BASE = f\"{RESTAPI_ADDRESS}/api\"\n",
    "\n",
    "# Path to workflows archive\n",
    "WORKFLOWS_TAR_GZ = Path(\"workflows.tar.gz\")\n",
    "\n",
    "# Experiment name (note the username_ prefix convention)\n",
    "EXPERIMENT_NAME = f\"{USERNAME}_roadsigns_yolo\"\n",
    "\n",
    "# Path to dataset\n",
    "data_path_roadsigns = \"../../../data/roadsigns/images\"\n",
    "weights_path_yolo = \"/home/jovyan/weights/yolo\"\n",
    "\n",
    "# Import third-party Python packages\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Import utils.py file\n",
    "import utils\n",
    "\n",
    "# Create random number generator\n",
    "rng = np.random.default_rng(54399264723942495723666216079516778448)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Road Signs dataset needed for this demo can be obtained here: https://makeml.app/datasets/road-signs.\n",
    "Object bounding boxes are provided in the PascalVOC format, which organizes the dataset into two folders:\n",
    "\n",
    "    annotations/  (xml files)\n",
    "    images/       (png files)\n",
    "\n",
    "The PascalVOC format uses filenames to associate images with their corresponding annotation.\n",
    "For example, the image file `images/road1.png` has an associated annotation file `annotations/road1.xml`.\n",
    "\n",
    "This dataset does not provide a testing dataset, so we need to create our own.\n",
    "We do this by stratifying over the number of objects and classes in the images and sampling 10% of the images without replacement within each of these groups.\n",
    "The exact train/test split we used is reported in the `roadsigns_train_test_split.csv` file.\n",
    "\n",
    "After performing our train/test split, our data is reorganized into the following folder structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    roadsigns\n",
    "    ├── testing\n",
    "    │   ├── annotations\n",
    "    │   └── images\n",
    "    └── training\n",
    "        ├── annotations\n",
    "        └── images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sampling of the training images to confirm that there aren't any problems,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n",
       "        \n",
       "            <figure style=\"margin: 5px !important;\">\n",
       "              <img src=\"../../../data/roadsigns/images/road333.png\" style=\"height: 140px\">\n",
       "              <figcaption style=\"font-size: 0.6em\">../../../data/roadsigns/images/road333.png</figcaption>\n",
       "            </figure>\n",
       "        \n",
       "            <figure style=\"margin: 5px !important;\">\n",
       "              <img src=\"../../../data/roadsigns/images/road423.png\" style=\"height: 140px\">\n",
       "              <figcaption style=\"font-size: 0.6em\">../../../data/roadsigns/images/road423.png</figcaption>\n",
       "            </figure>\n",
       "        \n",
       "            <figure style=\"margin: 5px !important;\">\n",
       "              <img src=\"../../../data/roadsigns/images/road560.png\" style=\"height: 140px\">\n",
       "              <figcaption style=\"font-size: 0.6em\">../../../data/roadsigns/images/road560.png</figcaption>\n",
       "            </figure>\n",
       "        \n",
       "            <figure style=\"margin: 5px !important;\">\n",
       "              <img src=\"../../../data/roadsigns/images/road61.png\" style=\"height: 140px\">\n",
       "              <figcaption style=\"font-size: 0.6em\">../../../data/roadsigns/images/road61.png</figcaption>\n",
       "            </figure>\n",
       "        \n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roadsigns_training_samples = sorted(\n",
    "    rng.choice(list(Path(data_path_roadsigns).glob(\"*.png\")), size=4).tolist()\n",
    ")\n",
    "utils.notebook_gallery(roadsigns_training_samples, row_height=\"140px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and run jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entrypoints that we will be running in this example are implemented in the Python source files under `src/` and the `MLproject` file.\n",
    "To run these entrypoints within the testbed architecture, we need to package those files up into an archive and submit it to the Testbed RESTful API to create a new job.\n",
    "For convenience, the `Makefile` provides a rule for creating the archive file for this example, just run `make workflows`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the endpoint, we will use a client class defined in the `utils.py` file that is able to connect with the Testbed RESTful API using the HTTP protocol.\n",
    "We connect using the client below, which uses the environment variable `AI_RESTAPI_URI` to figure out how to connect to the Testbed RESTful API,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client = utils.SecuringAIClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to register an experiment under which to collect our job runs.\n",
    "The code below checks if the relevant experiment exists.\n",
    "If it does, then it just returns info about the experiment, if it doesn't, it then registers the new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_experiment = restapi_client.get_experiment_by_name(name=EXPERIMENT_NAME)\n",
    "\n",
    "if response_experiment is None or \"Not Found\" in response_experiment.get(\"message\", []):\n",
    "    response_experiment = restapi_client.register_experiment(name=EXPERIMENT_NAME)\n",
    "\n",
    "response_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check which queues are available for running our jobs to make sure that the resources that we need are available.\n",
    "The code below queries the Testbed API and returns a list of active queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restapi_client.list_queues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_queue = restapi_client.get_queue_by_name(name=\"tensorflow_cpu\")\n",
    "\n",
    "if response_queue is None or \"Not Found\" in response_queue.get(\"message\", []):\n",
    "    response_queue = restapi_client.register_queue(name=\"tensorflow_cpu\")\n",
    "\n",
    "response_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example also makes use of the `evaluation` and `roadsigns_yolo_estimators` packages stored locally under the `task-plugins/securingai_custom` directory.\n",
    "To register these custom task plugins, we first need to package them up into an archive.\n",
    "For convenience, the `Makefile` provides a rule for creating the custom task plugins archive file, just run `make custom-plugins`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create the workflows.tar.gz file\n",
    "make custom-plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the custom task plugin packages are packaged into archive files, next we register them by uploading the files to the REST API.\n",
    "Note that we need to provide the name to use for custom task plugin package, and this name must be unique under the custom task plugins namespace.\n",
    "For a full list of the custom task plugins, use `restapi_client.restapi_client.list_custom_task_plugins()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluation_custom_plugins = restapi_client.get_custom_task_plugin(name=\"evaluation\")\n",
    "\n",
    "if response_evaluation_custom_plugins is None or \"Not Found\" in response_evaluation_custom_plugins.get(\"message\", []):\n",
    "    response_evaluation_custom_plugins = restapi_client.upload_custom_plugin_package(\n",
    "        custom_plugin_name=\"evaluation\",\n",
    "        custom_plugin_file=CUSTOM_PLUGINS_EVALUATION_TAR_GZ,\n",
    "    )\n",
    "\n",
    "response_evaluation_custom_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_roadsigns_custom_plugins = restapi_client.get_custom_task_plugin(name=\"roadsigns_yolo_estimators\")\n",
    "\n",
    "if response_roadsigns_custom_plugins is None or \"Not Found\" in response_roadsigns_custom_plugins.get(\"message\", []):\n",
    "    response_roadsigns_custom_plugins = restapi_client.upload_custom_plugin_package(\n",
    "        custom_roadsigns_plugin_name=\"roadsigns_yolo_estimators\",\n",
    "        custom_roadsigns_plugin_file=CUSTOM_PLUGINS_ROADSIGNS_YOLO_ESTIMATORS_TAR_GZ,\n",
    "    )\n",
    "\n",
    "response_roadsigns_custom_plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at any point you need to update one or more files within the `evaluation` or `roadsigns_yolo_estimators` packages, you will need to unregister/delete the custom task plugin first using the REST API.\n",
    "This can be done as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Delete the 'evaluation' custom task plugin package\n",
    "restapi_client.delete_custom_task_plugin(name=\"evaluation\")\n",
    "\n",
    "# Delete the `roadsigns_yolo_estimators` package\n",
    "restapi_client.delete_custom_task_plugin(name=\"roadsigns_yolo_estimators\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have deleted the task plugin in the testbed, re-run the `make custom-plugins` code block to update the package archive, then upload the updated plugin by re-running the `restapi_client.upload_custom_plugin_packge` block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to use transfer learning to update the object detection layers in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Submit transfer learning job for the mobilenet_v2 + yolo network architecture\n",
    "response_mobilenet_v2_yolo_transfer_learn = restapi_client.submit_job(\n",
    "    workflows_file=WORKFLOWS_TAR_GZ,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    entry_point=\"transfer_learn\",\n",
    "    entry_point_kwargs=\" \".join([\n",
    "        \"-P data_dir=/nfs/data/roadsigns/training\",\n",
    "        \"-P model_architecture=mobilenet_v2\",\n",
    "        \"-P epochs=300\",\n",
    "        \"-P batch_size=32\",\n",
    "        f\"-P register_model_name={EXPERIMENT_NAME}_mobilenet_v2_yolo\",\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Transfer learning job for MobileNet V2 + YOLO neural network submitted\")\n",
    "print(\"\")\n",
    "pprint.pprint(response_mobilenet_v2_yolo_transfer_learn)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edee40310913f16e2ca02c1d37887bcb7f07f00399ca119bb7e27de7d632ea99"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
